{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d967b9-7872-4873-a2f1-9cbf00ba7d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8888581c9dc04f7e92c97035c9d31751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  19%|#9        | 1.03G/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-1.3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f32c110-77a0-485e-97eb-6597dd227c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete! Saved as dataset.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load the original dataset\n",
    "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "# Convert intent-based dataset to input-output pairs\n",
    "for intent in data[\"intents\"]:\n",
    "    for text in intent[\"text\"]:  # Loop through each user question\n",
    "        if intent[\"responses\"]:  # Ensure there are responses\n",
    "            response = random.choice(intent[\"responses\"])  # Pick one relevant response\n",
    "            preprocessed_data.append({\"input\": text, \"output\": response})\n",
    "\n",
    "# Save as JSON Lines (JSONL)\n",
    "with open(\"dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in preprocessed_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Preprocessing complete! Saved as dataset.jsonl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4328c7e8-b189-4a92-9b3c-aa7a63af1a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698b310d13554a4e9ccbe8b75bae5aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc1c912bf724592a9b5d00c57ad68dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix padding issue\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"input\"]]\n",
    "    targets = [r for r in examples[\"output\"]]\n",
    "    \n",
    "    # Tokenize inputs and outputs\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f0945-b33f-4cab-8834-ba4027f0860d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 21/156 1:51:54 < 13:15:05, 0.00 it/s, Epoch 0.38/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-gpt-neo\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-gpt-neo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43806b38-2767-4ded-9e5e-7e2027a25217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=128,\n",
    "        do_sample=True,  # Enable sampling\n",
    "        temperature=0.9,  # Encourage creativity\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,  # Reduce repetition\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95915354-855f-4225-9a60-df02489d6513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the admission requirements?\n",
      "Chatbot: What are the admission requirements?\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot\n",
    "user_input = \"What are the admission requirements?\"\n",
    "response = generate_response(user_input)\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"Chatbot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2345f7a-9255-40be-9e29-37686013ce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the admission requirements?\n",
      "Chatbot: What are the admission requirements?\n",
      "\n",
      "User: Do I need to submit SAT scores for admission?\n",
      "Chatbot: Do I need to submit SAT scores for admission?\n",
      "\n",
      "User: What is the minimum GPA required for admission?\n",
      "Chatbot: What is the minimum GPA required for admission?\n",
      "\n",
      "User: How much is the tuition fee?\n",
      "Chatbot: How much is the tuition fee?\n",
      "\n",
      "User: What courses are offered in the Computer Science program?\n",
      "Chatbot: What courses are offered in the Computer Science program?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with different inputs\n",
    "questions = [\n",
    "    \"What are the admission requirements?\",\n",
    "    \"Do I need to submit SAT scores for admission?\",\n",
    "    \"What is the minimum GPA required for admission?\",\n",
    "    \"How much is the tuition fee?\",\n",
    "    \"What courses are offered in the Computer Science program?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    response = generate_response(question)\n",
    "    print(f\"User: {question}\")\n",
    "    print(f\"Chatbot: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04330693-5e01-448b-a9b0-29fad4df4817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Hi', 'output': 'Hello!'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])  # Check if input-output pairs are correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d82c6-6ece-4938-822b-0dad40d4afde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
